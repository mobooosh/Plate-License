{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24050dfb-e631-49e0-8594-fb0869de289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52432a9-5251-4a31-b93a-ea408b412c9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f2ae9f-7bfd-4f5b-9ee1-d649b896653f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eb94722-6a8c-4ee9-aa03-5ae9f8c9f6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a442b64-feea-4aac-926c-9fd50d1a0546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58536fcc-261d-4f25-a6ca-abd1a4c98e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = tf.config.list_physical_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44dac6e2-5b91-4b25-9bc1-f7697af7b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Lambda, Concatenate, Reshape, Conv2D, MaxPooling2D, Dropout, Dense, Flatten, LSTM, Input, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.layers import Embedding\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import os\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "354525d0-856b-49e4-97cd-e512d2569bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"mergedData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d8897b5-dae2-4d7a-83a5-a8c9c500ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.listdir(train_dir)\n",
    "lbl = []\n",
    "for e in path:\n",
    "    lbl.append(e.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edfd7c5c-25e9-447e-a0f7-d254b7c56208",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = set(char for label in lbl for char in label)\n",
    "characters.add('G')\n",
    "characters.add('E')\n",
    "\n",
    "characters = sorted(list(characters))\n",
    "\n",
    "\n",
    "char_to_num = layers.StringLookup(\n",
    "    vocabulary=list(characters), mask_token=None\n",
    ")\n",
    "num_to_char = layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "994d8952-8349-4664-a7a2-62f81049f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = char_to_num.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e0a17aa-96b7-4331-acbd-5c53af5716eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "tokenized_lbls = []\n",
    "for i in path:\n",
    "    image_paths.append(os.path.join(train_dir, i))\n",
    "    temp = []\n",
    "    for char in i.split('_')[0]:\n",
    "        temp.append(int(char_to_num(char)))\n",
    "    tokenized_lbls.append([int(char_to_num('G'))] + temp + [int(char_to_num('E'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b58123f-e00d-4f40-989d-33f8b1fb3a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.optimizer.set_jit(True)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "desired_height = 65\n",
    "desired_width = 256\n",
    "\n",
    "def normalization(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "def load_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    image = normalization(image)    \n",
    "    image = tf.image.resize(image, [desired_height, desired_width])\n",
    "    return image\n",
    "\n",
    "# def data_augmentation(image):\n",
    "#     image = tf.image.adjust_contrast(image, 0.4)\n",
    "#     image = tf.image.adjust_brightness(image, 0.3)\n",
    "#     image = tf.image.adjust_saturation(image, 0.3)\n",
    "#     return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_image_label(path, label):\n",
    "    image = load_image(path)\n",
    "    image = tf.transpose(image, perm=[1, 0, 2])\n",
    "    # image = data_augmentation(image)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return (image, label), label[1:]\n",
    "    \n",
    "SHUFFLE_BUFFER_SIZE = 256\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9aef425-c201-49f5-b2ce-d7a6b163e902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  8519\n",
      "train size:  7667\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, tokenized_lbls))\n",
    "\n",
    "\n",
    "validation_split = 0.1\n",
    "DATASET_SIZE = len(list(dataset))\n",
    "print(\"Dataset size: \", DATASET_SIZE)\n",
    "train_size = int((1-validation_split) * DATASET_SIZE)\n",
    "print(\"train size: \", train_size)\n",
    "train_dataset = dataset.take(train_size)\n",
    "validation_dataset = dataset.skip(train_size)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(load_image_label, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.prefetch(AUTOTUNE)\n",
    "\n",
    "validation_dataset = validation_dataset.map(load_image_label, num_parallel_calls=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE, num_parallel_calls=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b7dac9a-755a-40b2-b728-1082e143f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Conv2D, MaxPool2D, GlobalAveragePooling1D, Input, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b39fcca2-25c8-48b7-a836-9ee886257777",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_UNITS = 256\n",
    "EMBEDDING_DIM = 128\n",
    "ATTENTION_UNITS = 8\n",
    "VOCAB_SIZE = len(dict_)\n",
    "START_TOKEN = char_to_num('G')\n",
    "END_TOKEN = char_to_num('E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d74a2e3a-1cb1-42d0-8346-4805cb9ab1b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base = ResNet50(include_top=False, input_shape=(desired_height, desired_width, 3))\n",
    "base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e11e416a-fcbd-4341-b208-1092200d4667",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = keras.models.Model(inputs=base.inputs, outputs=base.get_layer('conv4_block6_out').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99cd457a-a96c-42cb-9083-a700d85996ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_shape = (desired_width // 16, (desired_height // 16) * 256)\n",
    "nclasses = VOCAB_SIZE\n",
    "\n",
    "# Encoder\n",
    "input_layer = tf.keras.Input(shape=(desired_width, desired_height, 3))\n",
    "conv2d1 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n",
    "pool1 = layers.MaxPooling2D((2, 2))(conv2d1)\n",
    "\n",
    "conv2d2 = layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "pool2 = layers.MaxPooling2D((2, 2))(conv2d2)\n",
    "\n",
    "conv2d3 = layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "pool3 = layers.MaxPooling2D((2, 2))(conv2d3)\n",
    "\n",
    "conv2d4 = layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n",
    "pool4 = layers.MaxPooling2D((2, 2))(conv2d4)\n",
    "\n",
    "reshape = layers.Reshape(target_shape=new_shape)(pool4)\n",
    "reshape = Dropout(0.4)(reshape)\n",
    "encoder_outputs, h, c = layers.LSTM(units=ENCODER_UNITS, return_sequences=True, return_state=True, dropout=0.2)(reshape)\n",
    "\n",
    "encoder = keras.models.Model(inputs=[input_layer], outputs=[encoder_outputs, h, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "931df390-1156-475a-be07-f6f34b84b6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 256, 65, 3)]      0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 256, 65, 32)       896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 128, 32, 32)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 128, 32, 64)       18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 64, 16, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 64, 16, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 32, 8, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 8, 256)        295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 16, 4, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 16, 1024)          0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 1024)          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 [(None, 16, 256),         1311744   \n",
      "                              (None, 256),                       \n",
      "                              (None, 256)]                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1700160 (6.49 MB)\n",
      "Trainable params: 1700160 (6.49 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "364ce857-4482-4a6c-b1e5-2dca7af03f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(keras.models.Model):\n",
    "    def __init__(self, attention_units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_units = attention_units\n",
    "\n",
    "        self.W1 = Dense(units=attention_units, use_bias=True)\n",
    "        self.W2 = Dense(units=attention_units, use_bias=True)\n",
    "        self.V = Dense(units=1, use_bias=True)\n",
    "\n",
    "    def call(self, encoder_outputs, decoder_state, training=False):\n",
    "        # Encoder_outputs: [Batch_size, num_sequences, encoder_units]\n",
    "        # decoder_state: [Batch_size, decoder_units]\n",
    "        decoder_state = tf.expand_dims(decoder_state, axis=1)\n",
    "        # e: [Batch_size, num_sequences, 1]\n",
    "        e = self.V(tf.nn.tanh(self.W1(encoder_outputs) + self.W2(decoder_state)))\n",
    "        alpha = tf.nn.softmax(e, axis=1)\n",
    "\n",
    "        # [Batch_size, encoder_units]\n",
    "        return tf.reduce_sum(alpha * encoder_outputs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b641695-c096-4034-885b-0de132bab8ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, 256)]                0         []                            \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)        [(None, 256)]                0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, None, 256)]          0         []                            \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)      (None, 512)                  0         ['input_5[0][0]',             \n",
      "                                                                     'input_6[0][0]']             \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " attention (Attention)       (None, 256)                  6169      ['input_4[0][0]',             \n",
      "                                                                     'tf.concat[0][0]']           \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, None, 128)            3968      ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda  (None, 1, 256)               0         ['attention[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)    (None, 1, 384)               0         ['embedding[0][0]',           \n",
      "                                                                     'tf.expand_dims[0][0]']      \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, 1, 256),             656384    ['tf.concat_1[0][0]',         \n",
      "                              (None, 256),                           'input_5[0][0]',             \n",
      "                              (None, 256)]                           'input_6[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 1, 256)               0         ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1, 31)                7967      ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 674488 (2.57 MB)\n",
      "Trainable params: 674488 (2.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs = keras.Input(shape=(None,))\n",
    "encoder_outputs = keras.Input(shape=(None, ENCODER_UNITS))\n",
    "h_inp = keras.Input(shape=(ENCODER_UNITS,))\n",
    "c_inp = keras.Input(shape=(ENCODER_UNITS,))\n",
    "\n",
    "attention = Attention(ATTENTION_UNITS)\n",
    "decoder_state = tf.concat([h_inp, c_inp], axis=-1)\n",
    "context = attention(encoder_outputs, decoder_state)\n",
    "context = tf.expand_dims(context, axis=1, name='concat_attention')\n",
    "\n",
    "# Embed: [Batch_size, 1, num_units]\n",
    "embed = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM)(decoder_inputs)\n",
    "context = tf.concat([embed, context], axis=-1)\n",
    "\n",
    "lstm_layer, h, c = LSTM(ENCODER_UNITS, return_sequences=True, return_state=True, dropout=0.2)(inputs=context, initial_state=[h_inp, c_inp])\n",
    "lstm_layer = Dropout(0.4)(lstm_layer)\n",
    "output = Dense(VOCAB_SIZE, activation='softmax')(lstm_layer)\n",
    "decoder = keras.models.Model(inputs=[decoder_inputs, encoder_outputs, h_inp, c_inp], outputs=[output, h, c])\n",
    "decoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac59dcda-07de-4fca-8977-a853043b5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(keras.models.Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        source, target = inputs\n",
    "        batch_size = tf.shape(source)[0]\n",
    "        encoder_outputs, h, c = self.encoder(source, training=training)\n",
    "        output_array = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        for i in range(1, tf.shape(target)[1]):\n",
    "            decoder_output, h, c = self.decoder([target[:, i - 1, tf.newaxis], encoder_outputs, h, c], training=training)\n",
    "            decoder_output = tf.squeeze(decoder_output, axis=1)\n",
    "            output_array = output_array.write(i - 1, decoder_output)\n",
    "\n",
    "        outputs = output_array.stack()\n",
    "        outputs = tf.transpose(outputs, perm=[1, 0, 2])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a723f1e-5d90-49f5-b9d9-b1be0cdede1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0d711de-f7d4-4e96-a2ea-2f44495d4cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_obj = keras.losses.SparseCategoricalCrossentropy()\n",
    "accuracy = keras.metrics.Accuracy()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "early_stopping_patience = 15\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",
    ")\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7,\n",
    "                              patience=3, min_lr=0.0001)\n",
    "\n",
    "callbacks = [keras.callbacks.TensorBoard(), early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f94e82a-a040-47be-a582-21e339ae9e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1719527348.010845   59301 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 45s 112ms/step - loss: 2.0952 - accuracy: 0.2730 - val_loss: 1.8148 - val_accuracy: 0.3170 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "240/240 [==============================] - 30s 126ms/step - loss: 1.8164 - accuracy: 0.3202 - val_loss: 1.7576 - val_accuracy: 0.3259 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "240/240 [==============================] - 17s 73ms/step - loss: 1.7645 - accuracy: 0.3272 - val_loss: 1.7230 - val_accuracy: 0.3337 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "240/240 [==============================] - 17s 73ms/step - loss: 1.7030 - accuracy: 0.3547 - val_loss: 1.6530 - val_accuracy: 0.3818 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "240/240 [==============================] - 18s 73ms/step - loss: 1.5864 - accuracy: 0.4129 - val_loss: 1.4389 - val_accuracy: 0.4704 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "240/240 [==============================] - 18s 73ms/step - loss: 1.3400 - accuracy: 0.5113 - val_loss: 1.0844 - val_accuracy: 0.5992 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "240/240 [==============================] - 17s 73ms/step - loss: 0.9892 - accuracy: 0.6333 - val_loss: 0.6907 - val_accuracy: 0.7491 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "240/240 [==============================] - 18s 73ms/step - loss: 0.6599 - accuracy: 0.7535 - val_loss: 0.4426 - val_accuracy: 0.8319 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "240/240 [==============================] - 18s 73ms/step - loss: 0.4675 - accuracy: 0.8197 - val_loss: 0.3206 - val_accuracy: 0.8760 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "240/240 [==============================] - 18s 73ms/step - loss: 0.3541 - accuracy: 0.8575 - val_loss: 0.2594 - val_accuracy: 0.8921 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "240/240 [==============================] - 18s 73ms/step - loss: 0.2757 - accuracy: 0.8865 - val_loss: 0.2171 - val_accuracy: 0.9078 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.2352 - accuracy: 0.9026 - val_loss: 0.1873 - val_accuracy: 0.9242 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "240/240 [==============================] - 18s 73ms/step - loss: 0.1937 - accuracy: 0.9229 - val_loss: 0.1516 - val_accuracy: 0.9489 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.1542 - accuracy: 0.9467 - val_loss: 0.1094 - val_accuracy: 0.9679 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "240/240 [==============================] - 18s 73ms/step - loss: 0.1171 - accuracy: 0.9629 - val_loss: 0.0792 - val_accuracy: 0.9791 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0907 - accuracy: 0.9721 - val_loss: 0.0754 - val_accuracy: 0.9799 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0728 - accuracy: 0.9779 - val_loss: 0.0665 - val_accuracy: 0.9833 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0606 - accuracy: 0.9824 - val_loss: 0.0604 - val_accuracy: 0.9853 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0498 - accuracy: 0.9856 - val_loss: 0.0586 - val_accuracy: 0.9868 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0466 - accuracy: 0.9866 - val_loss: 0.0583 - val_accuracy: 0.9857 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0403 - accuracy: 0.9882 - val_loss: 0.0507 - val_accuracy: 0.9877 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0377 - accuracy: 0.9889 - val_loss: 0.0552 - val_accuracy: 0.9875 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0372 - accuracy: 0.9891 - val_loss: 0.0550 - val_accuracy: 0.9863 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0381 - accuracy: 0.9886 - val_loss: 0.0442 - val_accuracy: 0.9889 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0326 - accuracy: 0.9901 - val_loss: 0.0472 - val_accuracy: 0.9889 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "240/240 [==============================] - 18s 75ms/step - loss: 0.0254 - accuracy: 0.9928 - val_loss: 0.0462 - val_accuracy: 0.9902 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0254 - accuracy: 0.9927 - val_loss: 0.0457 - val_accuracy: 0.9902 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "240/240 [==============================] - 18s 75ms/step - loss: 0.0157 - accuracy: 0.9958 - val_loss: 0.0382 - val_accuracy: 0.9911 - lr: 7.0000e-04\n",
      "Epoch 29/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0143 - accuracy: 0.9962 - val_loss: 0.0444 - val_accuracy: 0.9883 - lr: 7.0000e-04\n",
      "Epoch 30/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0158 - accuracy: 0.9957 - val_loss: 0.0433 - val_accuracy: 0.9906 - lr: 7.0000e-04\n",
      "Epoch 31/50\n",
      "240/240 [==============================] - 18s 75ms/step - loss: 0.0135 - accuracy: 0.9960 - val_loss: 0.0408 - val_accuracy: 0.9911 - lr: 7.0000e-04\n",
      "Epoch 32/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0112 - accuracy: 0.9971 - val_loss: 0.0359 - val_accuracy: 0.9919 - lr: 4.9000e-04\n",
      "Epoch 33/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0086 - accuracy: 0.9978 - val_loss: 0.0384 - val_accuracy: 0.9915 - lr: 4.9000e-04\n",
      "Epoch 34/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0095 - accuracy: 0.9976 - val_loss: 0.0403 - val_accuracy: 0.9914 - lr: 4.9000e-04\n",
      "Epoch 35/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0092 - accuracy: 0.9977 - val_loss: 0.0380 - val_accuracy: 0.9922 - lr: 4.9000e-04\n",
      "Epoch 36/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0368 - val_accuracy: 0.9914 - lr: 3.4300e-04\n",
      "Epoch 37/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0050 - accuracy: 0.9989 - val_loss: 0.0425 - val_accuracy: 0.9910 - lr: 3.4300e-04\n",
      "Epoch 38/50\n",
      "240/240 [==============================] - 18s 75ms/step - loss: 0.0061 - accuracy: 0.9986 - val_loss: 0.0428 - val_accuracy: 0.9906 - lr: 3.4300e-04\n",
      "Epoch 39/50\n",
      "240/240 [==============================] - 18s 75ms/step - loss: 0.0045 - accuracy: 0.9989 - val_loss: 0.0380 - val_accuracy: 0.9919 - lr: 2.4010e-04\n",
      "Epoch 40/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.0400 - val_accuracy: 0.9915 - lr: 2.4010e-04\n",
      "Epoch 41/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.0391 - val_accuracy: 0.9926 - lr: 2.4010e-04\n",
      "Epoch 42/50\n",
      "240/240 [==============================] - 18s 75ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0342 - val_accuracy: 0.9928 - lr: 1.6807e-04\n",
      "Epoch 43/50\n",
      "240/240 [==============================] - 18s 75ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0395 - val_accuracy: 0.9915 - lr: 1.6807e-04\n",
      "Epoch 44/50\n",
      "240/240 [==============================] - 18s 75ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0374 - val_accuracy: 0.9919 - lr: 1.6807e-04\n",
      "Epoch 45/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0380 - val_accuracy: 0.9927 - lr: 1.6807e-04\n",
      "Epoch 46/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.0336 - val_accuracy: 0.9928 - lr: 1.1765e-04\n",
      "Epoch 47/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0368 - val_accuracy: 0.9930 - lr: 1.1765e-04\n",
      "Epoch 48/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0353 - val_accuracy: 0.9926 - lr: 1.1765e-04\n",
      "Epoch 49/50\n",
      "240/240 [==============================] - 18s 74ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0344 - val_accuracy: 0.9932 - lr: 1.1765e-04\n",
      "Epoch 50/50\n",
      "240/240 [==============================] - 18s 75ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0376 - val_accuracy: 0.9927 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff7f45416d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=50, validation_data=validation_dataset, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fda7219-d7f3-446d-988b-4b9bc652c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "test_image = cv2.imread('4.JPG', 0)\n",
    "test_image = cv2.cvtColor(test_image, cv2.COLOR_GRAY2RGB)\n",
    "resized_image = cv2.resize(test_image, (256, 65))\n",
    "resized_image = tf.transpose(resized_image, perm=[1, 0, 2])\n",
    "resized_image = resized_image[np.newaxis, ...]\n",
    "resized_image = tf.cast(resized_image, tf.float32)\n",
    "resized_image = resized_image / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06f45270-68b0-4f5d-8c0b-4ca84b46a67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2], shape=(1,), dtype=int64)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n",
      "tf.Tensor([11], shape=(1,), dtype=int64)\n",
      "tf.Tensor([10], shape=(1,), dtype=int64)\n",
      "tf.Tensor([3], shape=(1,), dtype=int64)\n",
      "tf.Tensor([5], shape=(1,), dtype=int64)\n",
      "tf.Tensor([3], shape=(1,), dtype=int64)\n",
      "tf.Tensor([4], shape=(1,), dtype=int64)\n",
      "tf.Tensor([15], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "encoder_outputs, h, c = encoder(resized_image, training=False)\n",
    "outputs = tf.expand_dims([START_TOKEN] * 1, axis=1)\n",
    "for i in range(9):\n",
    "    dec_inp = outputs[:, -1]\n",
    "    dec_inp = dec_inp[:, tf.newaxis]\n",
    "    output, h, c = decoder([dec_inp, encoder_outputs, h, c], training=False)\n",
    "    output = tf.argmax(output[:, 0], axis=-1)\n",
    "    outputs = tf.concat([outputs, output[tf.newaxis, ...]], axis=-1)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e76654a3-fe46-4b19-a3e8-47fb6036d426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11A92423'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([num_to_char(x).numpy().decode('utf-8') for x in outputs.numpy()[0]][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b74f06be-2795-4942-a685-81b16ab71a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "INFO:tensorflow:Assets written to: TForcingAttentionModel.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TForcingAttentionModel.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:absl:<__main__.Attention object at 0x7ff7f4502cd0> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.save('TForcingAttentionModel.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc4ac74a-9779-4680-b980-740fe5c0befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = tf.keras.models.load_model('./attention_model_verygood.tf/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d93989-ef7a-4940-b724-f2f5e8b9f215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
