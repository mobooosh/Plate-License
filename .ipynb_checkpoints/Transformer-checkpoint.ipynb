{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be0ccb-606e-48ef-af2d-b8740d23ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Add, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming you have image_paths and captions lists\n",
    "image_paths = [...]  # List of image file paths\n",
    "captions = [...]     # List of corresponding captions\n",
    "\n",
    "# Tokenize the captions\n",
    "tokenizer = Tokenizer(char_level=True, filters='', lower=True)\n",
    "tokenizer.fit_on_texts(captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max(len(caption) for caption in captions)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(captions)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Split the data\n",
    "train_image_paths, val_image_paths, train_captions, val_captions = train_test_split(\n",
    "    image_paths, padded_sequences, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load and preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path).resize((224, 224))\n",
    "    img = np.array(img) / 255.0\n",
    "    return img\n",
    "\n",
    "train_images = np.array([preprocess_image(path) for path in train_image_paths])\n",
    "val_images = np.array([preprocess_image(path) for path in val_image_paths])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56a5c6-aa30-4702-9ab9-63573abd00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model for feature extraction\n",
    "base_model = ResNet50(include_top=False, weights='imagenet')\n",
    "cnn_output = base_model.output\n",
    "cnn_output = tf.keras.layers.GlobalAveragePooling2D()(cnn_output)\n",
    "cnn_model = Model(inputs=base_model.input, outputs=cnn_output)\n",
    "\n",
    "# Transformer Decoder with Attention\n",
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, max_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = self.positional_encoding(max_length, d_model)\n",
    "        self.dec_layers = [tf.keras.layers.MultiHeadAttention(num_heads, d_model) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def positional_encoding(self, max_length, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(max_length)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def call(self, x, training, look_ahead_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(len(self.dec_layers)):\n",
    "            x, block = self.dec_layers[i](x, x, x, attention_mask=look_ahead_mask, return_attention_scores=True)\n",
    "            attention_weights[f'decoder_layer{i+1}_block'] = block\n",
    "\n",
    "        output = self.final_layer(x)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# Combine the models\n",
    "image_input = Input(shape=(224, 224, 3))\n",
    "image_features = cnn_model(image_input)\n",
    "\n",
    "caption_input = Input(shape=(max_length,))\n",
    "caption_embedding = Embedding(vocab_size, 256)(caption_input)\n",
    "\n",
    "decoder_output, _ = TransformerDecoder(vocab_size, 256, 8, 6, max_length)(caption_embedding)\n",
    "\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder_output)\n",
    "\n",
    "model = Model(inputs=[image_input, caption_input], outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb7e5ae-e577-4ea2-8d19-750b08776e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Prepare data generators\n",
    "def data_generator(image_paths, captions, batch_size):\n",
    "    num_samples = len(image_paths)\n",
    "    while True:\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_images = np.array([preprocess_image(path) for path in image_paths[offset:offset+batch_size]])\n",
    "            batch_captions = captions[offset:offset+batch_size]\n",
    "            yield [batch_images, batch_captions[:, :-1]], batch_captions[:, 1:]\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "train_steps = len(train_image_paths) // batch_size\n",
    "val_steps = len(val_image_paths) // batch_size\n",
    "\n",
    "model.fit(\n",
    "    data_generator(train_image_paths, train_captions, batch_size),\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=data_generator(val_image_paths, val_captions, batch_size),\n",
    "    validation_steps=val_steps,\n",
    "    epochs=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b45ea-ba07-45e4-9089-70665dcc94db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23ad585-88ea-45bc-a1a4-a3c4bcf0790b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22221d2d-1144-41eb-9ced-97c93a3c2b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
